{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Assignment 4\n","## Lercio headlines generator\n","\n","Implementation and training of a character based RNN in pytorch to generate headlines in the style of the satirical news website [Lercio](https://www.lercio.it/)."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Environment setup and execution instructions\n","The project depends on the following libraries:\n","\n","pytorch: ML library for python\n","unidecode: used to convert unicode characters to the closest ASCII representation\n","tqdm: used to display progress bars\n","matplotlib: loss graph plotting\n","\n","The project is contained in a single jupyter notebook, all dependencies are installed automatically (except for pytorch, which is present in Colab but must be installed manually in a local environment).\n","Some pretrained models are downloaded automatically to show the results.\n","\n","The `EXECUTE_TRAINING` flag can be used to disable training and only load the pre-trained models.\n","\n","If the training results in a CUDA out-of-memory error, the batch size can be reduced by increasing the value of `splits`"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["EXECUTE_TRAINING = False\n","\n","# Comment if running locally\n","!pip install unidecode tqdm matplotlib \n","\n","# Uncomment if running locally\n","# %pip install --upgrade jupyter ipywidgets unidecode tqdm matplotlib\n","# If local, run this command after installing the requirements (enable tqdm widget to work properly)\n","# jupyter nbextension enable --py widgetsnbextension\n","\n","!rm lercio_headlines.csv\n","!wget --no-check-certificate https://elearning.di.unipi.it/pluginfile.php/46562/mod_assign/intro/lercio_headlines.csv"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Model definition\n","\n","The model was somewhat inspired by the [Char-RNN](https://github.com/spro/char-rnn.pytorch).\n","It is a simple RNN module based on GRU cells. It takes as input a batch of characters (encoded as one-hot vectors), passes them through `n_layers` layers of GRU, and after a fully connected layer, outputs a probability distribution over the characters (SoftMax)."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3841,"status":"ok","timestamp":1683204836204,"user":{"displayName":"Simone Ianniciello","userId":"14851307217949943336"},"user_tz":-120},"id":"UyncjAc8VezX"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","class CharRNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size, n_layers=1, dropout = 0):\n","        super(CharRNN, self).__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.n_layers = n_layers\n","\n","        self.rnn = nn.GRU(input_size, hidden_size, n_layers, dropout=dropout, batch_first=True).to(DEVICE)\n","        self.fc = nn.Linear(hidden_size, output_size).to(DEVICE)\n","        self.softmax = nn.LogSoftmax(dim=2).to(DEVICE)\n","        self.losses = None\n","\n","    def forward(self, input, hidden):\n","        output, hidden = self.rnn(input, hidden)\n","        output = self.fc(output)\n","        output = self.softmax(output)\n","        return output, hidden\n","\n","    # Initialize hidden state (used before forward pass for each mini-batch)\n","    def init_hidden(self, batch_size):\n","        return Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size)).to(DEVICE)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Data loading and preprocessing\n","\n","The data is loaded from the CSV, treating each row as a separate headline. In order to remove special characters, each line is passed through unidecode, which takes the raw text and returns a string with all non-standard characters replaced with their closest standard equivalent.\n","Then a dictionary `vocab` is created, mapping each character to a unique index."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":781,"status":"ok","timestamp":1683204836978,"user":{"displayName":"Simone Ianniciello","userId":"14851307217949943336"},"user_tz":-120},"id":"zJetCmpSVezZ","outputId":"9b593879-02ad-4286-bdd1-1ec2280cca28"},"outputs":[],"source":["from unidecode import unidecode\n","\n","def readLines(filename):\n","    with open(filename, 'r', encoding='utf-8') as f:\n","        return [unidecode(line) for line in f]\n","    \n","# Read the list of headlines from the csv\n","headlines = readLines('./lercio_headlines.csv')\n","headlines = sorted(headlines, key=len, reverse=True)\n","\n","# Create a list of all the unique letters in the headlines\n","all_letters_list = list(set(''.join(headlines))) + ['✕']\n","all_letters_list.sort()\n","all_letters = ''.join(all_letters_list)\n","num_letters = len(all_letters)\n","\n","# Create a dictionary mapping each letter to the corresponding index in the list of all letters\n","vocab = {all_letters[i]: i for i in range(num_letters)}"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Input tensors creation\n","\n","In order to speed up training, a batched approach is used. Since the headlines sequences are of different lengths, the sequences are padded with a special character `✕` not present in the text. As a result of this, two important tensors are created: `headlines_tensor` and `headlines_one_hot`:\n","- `headlines_tensor` is a tensor of shape `(batch_size, max_headline_length)`, containing the indices of the characters in the headlines.\n","- `headlines_one_hot` is a tensor of shape `(batch_size, max_headline_length, vocab_size)`, containing the one-hot encoding of the characters in the headlines.\n","The first one is needed in order to compute the loss, while the second one is needed as input to the model."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5963,"status":"ok","timestamp":1683204842936,"user":{"displayName":"Simone Ianniciello","userId":"14851307217949943336"},"user_tz":-120},"id":"kTPvzMXAVezb"},"outputs":[],"source":["# Convert headlines to tensors of character indices (vocab)\n","headlines_tensor_list = [torch.tensor([vocab[letter] for letter in headline]) for headline in headlines]\n","# Pad headlines tensor to common length\n","headlines_tensor = nn.utils.rnn.pad_sequence(headlines_tensor_list, batch_first=True, padding_value=vocab['✕']).to(DEVICE)\n","# Convert the padded dataset to one-hot encoding\n","headlines_one_hot = nn.functional.one_hot(headlines_tensor, num_classes=num_letters).to(torch.float32).to(DEVICE)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Sampling\n","\n","Two sampling methods are implemented: `generate` and `generate_normal`\n","- `generate` is just given a starting string and a single temperature value and keeps generating characters until it reaches `max_len` characters or the stop character `\\n` is encountered.\n","- `generate_normal` is given instead a starting string, and two parameters `mean_temp` and `std_temp`; for each character, the temperature is sampled from a normal distribution. The generation stops with the same criteria as before."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1683204842937,"user":{"displayName":"Simone Ianniciello","userId":"14851307217949943336"},"user_tz":-120},"id":"d1P-cLGqVezc"},"outputs":[],"source":["import numpy as np\n","\n","# Given a line of text, convert it to the respective one-hot encoding\n","def input_to_onehot(line):\n","    return nn.functional.one_hot(torch.tensor([vocab[c] for c in line]), num_letters).to(torch.float32).to(DEVICE).view(1, -1, num_letters)\n","\n","@torch.no_grad()\n","def generate(model, start_string, temperature = 1, max_len = 200):\n","    input = input_to_onehot(start_string)\n","    hidden = model.init_hidden(1)\n","\n","    output_name = start_string\n","\n","    for j in range(input.shape[1] - 1):\n","        output, hidden = model(input[:, j, :].view(1, 1, num_letters), hidden)\n","    for i in range(max_len):\n","        output, hidden = model(input[0, -1].view(1, 1, num_letters), hidden)\n","        if temperature == 0:\n","            topi = output.data.view(-1).argmax()\n","        else:\n","            output_dist = output.data.view(-1).div(temperature).exp()\n","            topi = torch.multinomial(output_dist, 1)[0]\n","        if topi == all_letters.find('\\n'):\n","            break\n","        else:\n","            letter = all_letters[topi]\n","            output_name += letter\n","        input = input_to_onehot(letter)\n","\n","    return output_name\n","\n","@torch.no_grad()\n","def generate_normal(model, start_string, mean_temp = 0, std_temp = 0, max_len = 200):\n","    input = input_to_onehot(start_string)\n","    hidden = model.init_hidden(1)\n","\n","    output_name = start_string\n","\n","    for j in range(input.shape[1] - 1):\n","        output, hidden = model(input[:, j, :].view(1, 1, num_letters), hidden)\n","    for i in range(max_len):\n","        output, hidden = model(input[0, -1].view(1, 1, num_letters), hidden)\n","\n","        temperature = np.random.normal(mean_temp, std_temp)\n","        temperature = abs(temperature)\n","\n","        # Since a too-low temperature would result in an exception given by limited numerical precision, this piece of code is encapsulated in a try-except block which handles the exception by getting the most probable character without temperature. \n","        try:\n","            output_dist = output.data.view(-1).div(temperature).exp()\n","            topi = torch.multinomial(output_dist, 1)[0]\n","        except RuntimeError:\n","            topi = output.data.view(-1).argmax()\n","        if topi == all_letters.find('\\n'):\n","            break\n","        else:\n","            letter = all_letters[topi]\n","            output_name += letter\n","        input = input_to_onehot(letter)\n","\n","    return output_name"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Training\n","\n","The model is trained using the Adam optimizer, with a starting learning rate of 0.01. A scheduler is used to decrease the learning rate as the training proceeds. \n","\n","The loss is computed using the `NLLLoss` function, using the `ignore_index` parameter set to the padding character (without this the model often would get stuck ouputting only padding chars).\n","\n","In order to fit the model in the GPU memory, the full set of headlines is split into mini-batches.\n","\n","Gradient clipping is used in order to mitigate some loss spikes.\n","\n","#### Training loop\n","The key points of the training loop are:\n","- The headlines are split in `splits` mini-batches. For each one:\n","    - The hidden states are initialized.\n","    - The loss (for the MB) is reset.\n","    - For each character of the sequences:\n","        - The model is fed the current character and the hidden states.\n","        - The loss is computed and added to the sum of losses.\n","    - The gradients are computed and clipped.\n","    - The optimizer is updated. (adam)\n","- The average loss for the epoch is computed.\n","- The scheduler is updated (learning rate).\n","\n","Every 50 epochs, some sample headlines are generated and printed.\n","\n","#### Results\n","\n","It was found empirically that an optimal loss value is around `0.20-0.10`. Above this values, the generated phrases are often not readable, while below this values the model tends to overfit and generate only phrases from the training set."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GT45L_zmVezc","outputId":"73e103b6-5b1f-47bf-d38e-e55d09670c03"},"outputs":[],"source":["import string\n","import random\n","import numpy as np\n","from pathlib import Path\n","from tqdm.notebook import tqdm\n","upper = string.ascii_uppercase\n","\n","# If no training requested, skip the cell\n","if EXECUTE_TRAINING:\n","\n","    # Remains of the teacher forcing tests\n","    # TFR_TARGET = 0.15\n","    # TFR_MAX = 1\n","    # TFR_MIN = 0.8\n","\n","    # Gradient clipping value\n","    CLIPPING = 0.1\n","\n","    padding = torch.tensor([vocab['✕']], device=DEVICE)\n","\n","    epochs = 1000\n","    splits = 5\n","\n","    PARAM = {\n","        'HIDDEN_SIZE': 250,\n","        'N_LAYERS': 2,\n","        'DROPOUT': 0\n","    }\n","\n","\n","    losses = []\n","    HIDDEN_SIZE = PARAM['HIDDEN_SIZE']\n","    N_LAYERS    = PARAM['N_LAYERS']\n","    DROPOUT     = PARAM['DROPOUT']\n","    base_name = f'H{HIDDEN_SIZE}_N{N_LAYERS}_D{DROPOUT:.2f}_'\n","\n","    rnn = CharRNN(num_letters, HIDDEN_SIZE, num_letters, N_LAYERS, DROPOUT).to(DEVICE)\n","    criterion = nn.NLLLoss(ignore_index=vocab['✕'])\n","\n","    # Initialize optimizer and lr scheduler with 0.01 initial lr, linear decay for 500 epochs up to 0.0001, then constant for the rest\n","    decoder_optimizer = torch.optim.RAdam(rnn.parameters(), lr=0.01)\n","    scheduler = torch.optim.lr_scheduler.LinearLR(decoder_optimizer, 1, 0.01, 500)\n","\n","    one_hot = headlines_one_hot\n","\n","    try:\n","        tfr = 1\n","        for epoch in tqdm(range(epochs), total=epochs):\n","            loss_perms = []\n","            # Split the data into `split` parts and train on each part\n","            perms = np.array_split(np.random.permutation(one_hot.shape[0]), splits)\n","            for perm in perms:\n","                input_tensor = one_hot[perm, :-1, :]\n","                output_tensor = headlines_tensor[perm, 1:]\n","                perm_size = input_tensor.shape[0]\n","                hidden = rnn.init_hidden(perm_size)\n","                rnn.zero_grad()\n","\n","                loss = 0\n","                character_tensor = input_tensor[:, 0:1, :]\n","                # Send sequience through the network one character at a time\n","                for c in range(input_tensor.shape[1]):\n","                    output, hidden = rnn(character_tensor, hidden)\n","                    # Since `criterion` is ignoring the padding character, once all the characters in the mini-batch are padding, the loss would be NaN.\n","                    # To avoid this, we set the loss to 0 manually.\n","                    if (output_tensor[:, c:c+1] == padding).all():\n","                        l = 0\n","                    else:\n","                        l = criterion(output.mT, output_tensor[:, c:c+1])\n","                    loss += l\n","                    # Teacher forcing (not used, always TRUE)\n","                    if random.random() < tfr:\n","                        character_tensor = input_tensor[:, c+1:c+2, :]\n","                    else:\n","                        character_tensor = output\n","                # Average the loss over the sequence length\n","                loss /= input_tensor.shape[1]\n","                    \n","                # Backpropagate and clip gradients\n","                loss.backward()\n","                torch.nn.utils.clip_grad_norm_(rnn.parameters(), CLIPPING)\n","\n","                decoder_optimizer.step()\n","                loss_perms.append(loss.data.item())\n","            loss_epoch = np.mean(loss_perms)\n","            losses.append(loss_epoch)\n","            scheduler.step()\n","            # tfr = min(1, TFR_MIN + (TFR_MAX - TFR_MIN) * (loss / TFR_TARGET))\n","\n","            # Every 50 epochs, generate some samples\n","            if epoch % 50 == 49:\n","                print(f'Epoch: {epoch},\\tloss: {loss_epoch}')\n","                start = random.choice(upper)\n","                print(f'T: 0.05  S: {generate(rnn, start, 0.05)}')\n","                print(f'T: 0.1   S: {generate(rnn, start, 0.1)}')\n","                print(f'T: 0.2   S: {generate(rnn, start, 0.2)}')\n","                print(f'T: .7±.4 S: {generate_normal(rnn, start, .7, .4)}')\n","    except KeyboardInterrupt:\n","        print('Interrupted')\n","    finally:\n","        # Once the training is done (or interrupted), save the model\n","        print('Saving model')\n","        model_name = f'H{HIDDEN_SIZE}_N{N_LAYERS}_D{DROPOUT:.2f}_E{epoch}_L{loss_epoch:.2f}_rnn.pt'\n","        rnn.losses = losses\n","        torch.save(rnn, model_name)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Sampling\n","\n","This cell picks every model saved to disk (workspace dir, <name>_rnn.pt) and generates some headlines for each uppercase letter of the alphabet at different temperature values.\n","\n","The sampling is done using both the `generate` and `generate_normal` functions.\n","Each UPPERCASE letter of the alphabet is used as a starting character, and the headlines are generated with both methods, and different parameters. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from matplotlib import pyplot as plt\n","\n","models = [x for x in Path('.').iterdir() if '_rnn.pt' in str(x)]\n","fig, ax = plt.subplots(figsize=(10, 7))\n","\n","for model in models:\n","    rnn = torch.load(model)\n","    rnn.train(False)\n","\n","    ax.plot(rnn.losses, label=model)\n","    fig.legend()\n","\n","    print(f'\\n################## {model} ##################\\n')\n","\n","    for U in upper:\n","        print(f'------ {U} ------')\n","        print(f'T: 0       S: {generate(rnn, U, 0)}')\n","        print(f'T: 0.1     S: {generate(rnn, U, 0.1)}')\n","        print(f'T: 0.5     S: {generate(rnn, U, 0.5)}')\n","        print(f'T: 0.7     S: {generate(rnn, U, 0.7)}')\n","        print(f'T: 0.9     S: {generate(rnn, U, 0.9)}')\n","        print(f'T: 0±.4    S: {generate_normal(rnn, U, 0, 0.4)}')\n","        print(f'T: 0.5±.4  S: {generate_normal(rnn, U, 0.5, 0.4)}')\n","        print(f'T: 1±.7    S: {generate_normal(rnn, U, 1, 0.7)}')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Results\n","\n","Above are shown some phrases generated with different models and at different temperatures.\n","Also shown are the loss curves for each one.\n","\n","It can be seen how much the hidden size is correlated with the loss: a simple change from 290 to 300 hidden units made in this case a difference of 0.10 in the loss, and a drastic difference in the quality of the generated phrases.\n","Also, the impact of dropout is not shown as i was not able to train a model with dropout which did not underfit.\n","From the loss curves, it can be seen that there are some spikes. The situation was much worse before the implementation of gradient clipping and the scheduling of the learning rate (to the point that before that, this results were unimaginable).\n","\n","##### H350_N2_D0.00_E1069_L0.05_rnn.pt\n","\n","This model was trained with an hidden size of 350, 2 layers, dropout of 0.00. The final loss was 0.05.\n","As can be seen, at low temperatures, the model outputs only phrases from the training set, while at higher temperatures it tends to generate less readable phrases. The `generate_normal` does not seem to improve the results.\n","\n","Examples:\n","\n","    \n","\n","##### H300_N2_D0.10_E609_L0.27_rnn.pt\n","\n","This model was trained with an hidden size of 300, 2 layers and a dropout of 0.10. The final loss was 0.27.\n","In this case, the vast majority of the generated phrases have no meaning whatsoever.\n","At low temperature the phrases are mostly composed of real words but with no meaning in their order, while at higher temperatures the generated words are mostly gibberish.\n","\n","Examples:\n","\n","\n","\n","##### H300_N2_D0.00_E725_L0.13_rnn.pt\n","\n","This one is a decent tradeoff between the others. It was trained with an hidden size of 300, 2 layers and a dropout of 0.00. The final loss was 0.13.\n","In the generated phrases, it is possible to find some phrases from the training set (mostly at low temperatures), and some nonsensical phrases.\n","However, it is still possible to see some (roughly) correct phrases which are not present in the training set.\n","\n","### Conclusions\n","\n","One big limiting factor of this model is the lack of a metric for evaluation: some candidates could be:\n","- The loss on a validation set (given the small size of the dataset and the high diversity in the headlines, I was not able to implement this properly)\n","- Proportion of generated words present in a dictionary; this however would not take into account the order of the words.\n","- A carefully crafted metric which would `reward` known (good) sequences of words while penalizing blatant copies of the training set in order to avoid overfitting.\n","- A human evaluation; which is roughly what I did, but it is not very reliable.\n","\n","### Funny results\n","\n","During the training, I saved the headlines which made me giggle. Here are some of them:\n","\n","_Di Maio rivela: \"Sono solo solo solo solo con le parte di parte di Barkandi\"_ (To read in the style of a Marco Mengoni's song)\n","\n","_La Chiesa apre ai matrimoni tra omosessuali di m***a_ (Like every good model, even mine had to be somewhat offensive, see Meta's BlenderBot3 racism scandal from Aug. 2022)\n","\n","_Trovato il gatto che ha mangiare i dati 1.008\"_ (The cat has eaten the data mid sentence, breaking the model)\n","\n","_LercioNews a Radio2 Social Club - 36 ottobre 2015_ (Apparently October has 36 days now)\n","\n","_Washington: estremista testimone di Geova che non parla inculittiva clandestini_ (This one was almost there)\n","\n","_Berlusconi scopre che andra al suo post di condominio del PD si arrende_ (this too)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":0}
